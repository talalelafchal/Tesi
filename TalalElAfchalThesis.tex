\documentclass[12pt,mscthesis]{usiinfthesis}

\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{float}
\usepackage{cleveref}
\graphicspath{ {figures/} }

\usepackage{listings}
\usepackage[autostyle]{csquotes} 

\lstdefinelanguage{algebra}
{morekeywords={import,sort,constructors,observers,transformers,axioms,if,
else,end},
sensitive=false,
morecomment=[l]{//s},
}



\title{Assessing Documents by Comprehension Effort } %compulsory
%\specialization{Dependable Distributed Systems}%optional
%\subtitle{Subtitle: Reinventing the World} %optional 
\author{Talal El Afchal} %compulsory
\begin{committee}
\advisor{Prof.}{Michele}{Lanza} %compulsory
\coadvisor{Prof.}{Gabriele}{Bavota}{} %optional
\coadvisor{Dr.}{Luca}{Ponzanelli}{}

\end{committee}
\Day{1} %compulsory
\Month{September} %compulsory
\Year{2017} %compulsory, put only the year
\place{Lugano} %compulsory

\dedication{To my beloved} %optional
\openepigraph{Your living is determined not so much by what life brings to you as by the attitude you bring to life; not so much by what happens to you as by the way your mind looks at what happens}{Gubran Khalil Gubran} %optional

%\makeindex %optional, also comment out \theindex at the end

\begin{document}

\maketitle %generates the titlepage, this is FIXED

\frontmatter %generates the frontmatter, this is FIXED

\begin{abstract}
Recommender systems for software engineering have become increasingly popular in recent years. These systems combine several methodologies to provide suggestions that meet the developer's needs. Recommender systems collect data from online resources as blogs, forums, Q\&A websites, and suggest documents or piece of code that are most likely helpful to the developers. However, these systems are not taking into consideration an important aspect as the comprehension effort, which may vary depending on the document familiarity and readability. Usually developers are more interested in documents which they are familiar with. By calculating the comprehension effort, the recommender system can complement the rank and suggest the most comprehensive and appropriate ones to the developer. In this work, we present our approach to calculating the comprehension effort, by creating a language model able to capture a document familiarity, that we combine with the document readability. 
\end{abstract}

\begin{acknowledgements}
\end{acknowledgements}

\tableofcontents 
\listoffigures %optional
\listoftables %optional

\mainmatter

\chapter{Introduction}

	\section{Context}
	The complexity of software systems is increasing and new technologies are introduced constantly \cite{Lehman:1985:PEP:7261}. Software developers often have to work with new technologies which they are not familiar with, and as increasingly more comes out, the amount of information that they need to know will increase. For example, Android\footnote{\url{https://www.android.com}} was introduced 10 years ago in 2007, and in 2008 the first version was released, and nowadays, there are 8 versions of Android. 
	

	When Android started to become popular, developers had to learn this technology and stay updated with each new release.
	They had to search how activities work in Android, and how to use several APIs to implement different tasks assigned to them.
	However, the process of searching the right piece of information as a tutorial or a documentation is time-consuming and requires considerable effort.
	For example, if an Android developer needs to use a new Android API that is completely new to her.She will start consulting web artifacts such as forums, blogs, questions and answers (Q\&A) websites, and API documentation \cite{Sim:2011:WSE:2063239.2063243}.


	The amount of resources is vast, a popular Q\&A websites as Stack Overflow contains a million of questions tagged as Android\footnote{\url{https://stackoverflow.com/questions/tagged/android}}. Github, one of the most popular version control system where developers can store their projects, hosts more than 500 thousand Android repositories\footnote{\url{https://github.com/search?utf8=✓&q=android&type=}}, with millions of lines of code which can be an important resource.


	A tool that helps developers to gather information among the available resources would ideally improve the search process, that would result in time saved for the developers.
	Similar tools to suggest items of interest already exists outside the context of software engineering, for example Amazon\footnote{\url{https://www.amazon.com}}, Ebay\footnote{\url{https://www.ebay.com}} and Netflix\footnote{\url{https://www.netflix.com/ch-en/}}, employ recommender systems to suggest their products.


	What is a recommender system? The proposed definition by the organizers of the ACM International Conference on Recommender Systems \footnote{\url{https://recsys.acm.org/recsys09}} is: \\

	  \blockquote{\textit{``Recommendation systems are software applications that aim to support users in their decision-making while interacting with large information spaces. They recommend items of interest to users based on preferences they have expressed, either explicitly or implicitly. The ever-expanding volume and increasing complexity of information [...] has therefore made such systems essential tools for users in a variety of information seeking [...] activities. Recommendation systems help overcome the information overload problem by exposing users to the most interesting items, and by offering novelty, surprise, and relevance.''}}

	  In the context of software engineering, programmers need other items useful to their scope as tutorials, code snippet and libraries. Recommender system that recommend similar items are known as recommender systems for software engineering (RSSE). \citet{RecommendationSystemsforSoftwareEngineering} introduced a RSSE definition:\\
	\blockquote{An RSSE is a software application that provides information items estimated to be valuable for a software engineering task in a given context.}
	

	There is a vast number of proposed RSSEs, some of them suggest relevant project artifacts or code examples \cite{Holmes:2005:USC:1062455.1062491} \cite{Cubranic:2003:HRP:776816.776866} \cite{Zimmermann:2004:MVH:998675.999460}. Other focused on suggesting relevant code samples, documents and discussions from the web resources \cite{Rahman:2015:RRS:2886444.2886471} \cite{Sawadsky:2011:FTC:1984708.1984722} \cite{Stylos:2006:MWT:1174509.1174678} \cite{10.1109/VLHCC.2012.6344497}. But none of them has considered the required effort to comprehend the suggested documents.


	Searching for documentation and tutorials is a crucial step in learning a new technology. Developers can find a bunch of online resources as blogs, forums, Q\&A websites, but the real challenge is to find the most suitable one for their needs. \citet{Singer-1997} reported in 1997 that the most frequent developer activity was code search, and \citet{Sadowski:2015} did a case study on how developers at Google search for code. They figured out that programmers are generally seeking answers to questions about how to use an API, what code does, why something is failing, or where the code is located. The interesting point in this study was the fact that most searches focus on code that is familiar, or somewhat familiar to the developers.
	Therefore, we believe that RSSE has to take into consideration the familiarity of a document when they suggest it to the developer. 

	
	Understanding a document is a cognitive process, and it depends on the human brain intelligence, but must probably if we are familiar with a subject, we will comprehend it with less effort. It is likely that a computer engineer comprehends a document explaining how to implement a sorting algorithm, with much less effort compared to a document that explains a constitutional law, and the reason is not that the algorithm is simple, but because a computer engineer is more familiar with sorting algorithms than law.
	

	In this thesis, we try to evaluate similar situations, for example:\\
	Given two documents that have approximately the same subject, how can we decide which one is easier to comprehend? To answer this question we need to introduce two concepts:
	\begin{itemize}
	\item \textbf{familiarity}: how much are developers familiar with the document content?
	\item \textbf{readability}: how difficult is it to read a given document?
	\end{itemize}
	The familiarity and readability metrics are important for us, since the comprehension effort can be derived from the document familiarity and readability.\\
	For example, if we want to use a tool that gives us a score to indicate which document requires less effort to be comprehended, where a higher score indicates a high effort, and we have two documents where, in the first one we have an Android task implemented in Android 7.1 and in the other document we have the same task implemented in Android 4.0, and the programmer is familiar with Android 7.1.
	We expect that the first document must have a lower score since logically it requires less effort to be comprehended by developers who are more familiar with Android 7.1.\\
	What if both documents have the same task, and both tasks are implemented with the Android 7.1 ? Which one will have a lower score? \\
	In this case, the document readability has a big impact on the comprehension effort: The developer would likely prefer to read the document with the best readability.\\
	
	\section{Objective and Results}
	Our main goal in this thesis is to assess documents by their comprehension effort, which can be leveraged by RSSE to improve their suggestions.\\
	To calculate the comprehension effort we need to find a way to calculate the familiarity, and we need to evaluate our approach to understand if it effectively works.\\

	To evaluate our approach we run two studies:

	 In the first study we evaluate our familiarity approach. We select a big set of Android documents, which represents the hypothetical developer knowledge, where a document can be a mix of code and natural language. Then we create a \emph{ Language Model} \cite{Hindle:2012:NS:2337223.2337322} and we train it with these documents (training set). In this way, we are able to simulate the context of a programmer who is familiar with Android. Once we trained the language model, we evaluate the familiarity of a given set of documents (testing set) that contains Android documents and other documents that are not related to Android.\\ 
	 The language model was able to evaluate the Android testing documents as thousand times more familiar than the rest of the testing documents.
	 In this experiment, the language model approach satisfied our expectation in capturing the document familiarity.\\

	 In the second study we evaluate our approach in assessing documents by the comprehension effort. In this study we ask programmers to read a set of tutorials, and then we give them a set of documents, where some documents are related to the tutorials and some are not, and we ask the developers to score the documents by the comprehension effort, and we compare their scores with our precomputed scores.
	 \dots 


	\section{Structure of the Thesis}
	This thesis consists of seven chapters: 
	\begin{enumerate}
	
		\item \textbf{Introduction}: In this chapter we introduce the thesis work.
		\item \textbf{State of the Art}: This chapter describes the existing related work as code search engines, and recommender systems.
		\item \textbf{Approach}: This chapter describes our approach to calculate the comprehension effort.
		\item \textbf{Study design}: This chapter discusses the research question, the data extraction process, the analysis method, and the replication package.
		\item \textbf{Result}: This chapter shows the results and their implication.
		\item \textbf{Threat to Validity}: This chapter describes the possible threats that could affect the results validity.
		\item \textbf{Conclusion}: This chapter summarizes our work, and presents some ideas for a possible future work, and concludes the thesis.
	\end{enumerate}
\chapter{State of the Art}
	In this chapter, we present the existing related work to the comprehension effort, and discuss other related tools, as code search engines and recommender systems.
\section{Program Comprehension}
	There are several studies on program comprehension and in this section we mention few of them, that are related to the thesis work. \\


	\citet{Corbi:1989:PUC:97118.97124} did a research on tools which could help programmers in two key areas: static analysis (reading the code) and dynamic analysis (running the code). In this paper Corbi describes how program understanding relates to software renewal, and he indicates that more than half of the programmer's task is in understanding the system, and the programmers are forced to spend most of their time studying the only really accurate representation of the system.	Corbi concludes the paper by mentioning that programmer training and tools should show or assist the programmer in combining  different kinds of information in ways which can support his or her in understanding the system being investigated, and they should not favor or force the use of only one way of gathering information about programs.
	This paper motivates us to investigate and develop a novel technique to calculate the comprehension effort.
	\newpage

	Another related work is one by \citet{Kushwaha:2006:ICI:1163514.1163533}  who claim that the required effort to understand a software depends on the difficulty in understanding the information, where the information is related to the number of operators and identifiers. They count the number of operators and identifiers per line of code and they multiply it by an associated weight of the identifier name ( 1 if the identifier name belongs to the problem domain, and 4 is the identifier name is selected arbitrarily). 


	They performed an experiment on 60 students, where 5 sample programs were given. One set of programs had meaningful identifier names related to the problem domain and the other used arbitrarily selected identifier name. They measured the required time to comprehend the program.


	The result showed that programs with arbitrarily selected identifier names required about 4 times the time to comprehend the programs compared to programs with meaningful identifiers.\\


	\citet{Scalabrino} presented a metric able to assess the understandability of a given code snippet . In their work they consider three types of metrics:
	\begin{enumerate}
		\item \textbf{Code-related metrics} are metrics related to the code, (e.g., cyclomatic complexity, LOC, the number of identifiers, line length).
		\item \textbf{Documentation-related metrics} capture the quality of the internal documentation of a snippet (e.g, comments readability, and identifiers consistency). 
		\item \textbf{Developer-related metrics} measure the programming experience of the developer in years, in any programming language.
	\end{enumerate}
	They analyze whether code-related, documentation-related, and developer- related metrics can be used to assess the understandability level of a snippet  code. The authors asked
	46 developers to carefully read and to fully understand eight code snippets. Participants could, in any moment select the option \textit{I understood the snippet} or \textit{I cannot understand the snippet}, and the time was monitored.
	Once the participant choosed \textit{I understood the snippet} option, the authors asked questions about the code snippet to verify the actual level of understanding.

	After an extensive statistical analysis \citet{Scalabrino} couldn't find a significant correlation between the considered metrics and the understandability of code snippets.
	They assumed that the code complexity has a big influence on the programmers' ability to understand the code, but they couldn't demonstrate it with a strong empirical evidence.They also mentioned that the code readability can have a direct impact on the understandability of the code.\\ As mentioned in the previous chapter, in this thesis we take in consideration the code readability to calculate the comprehension effort.\\

	\citet{Buse2010} introduced a code readability metric, and they investigate its relation to software quality. A part of their work was to run an experiment which compared readability of the code to the cyclomatic complexity, and they were able to show that code readability is significantly independent of the traditional code complexity.


	In this experiment, 120 developers were asked to individually score a sequence of 100 code snippets, based on their personal estimation of readability. From the result, they determined which code features were predictive of readability, and they construct a readability model. They also tested the model performance on ten different classifiers, and on average the model classified correctly between 75\% and 80\% of the snippets.They found that factors like \textit{average line length} and, \textit{average number of identifiers per line} are very important to readability.In this thesis we use \citet{Buse2010} approach to calculating code readability.

	\section{Semantic code search and code search engines }

	\citet{Reiss:2009:SCS:1555001.1555040} presented a tool that generates a specific functions or classes from the open source repositories, where these classes meet user's specifications. This tool uses the user's input, as keywords and other constraints, and suggests codes that meet the user's needs.


	The tool takes a set of candidate solutions, and it transforms it into a more appropriate set. Both static and dynamic specifications can be used. This tool main goal is to satisfy the user's constraint, but it does not take into consideration the user's experience and the comprehension effort.\\
	

	\citet{Thummalapenta:2007:PPA:1321631.1321663} presented \emph{PARSEWeb} a tool similar to \citet{Reiss:2009:SCS:1555001.1555040}, where they collect code from public sources and search engine suggest it to the programmer based on their input query.\\ In the query, the programmers have to specify the \textit{source object type} and the \textit{destination object type}. For example: QueueConnectionFactory \rightarrow QueueSender\\
	

	\citet{McMillan:2011:FRF:1985793.1986032} created an application search system called \textit{Exemplar}, which reduce the mismatch between the high-level intent reflected in the descriptions of software and low-level implementation details.

	 \textit{Exemplar} differs from the traditional search engine that matches the keywords, by matching keywords with the descriptions of the various API calls in help documents.
	\textit{Exemplar} has three components of Ranking: 
	\begin{enumerate}
		\item \textbf{WOS} a component that computes a score based on word occurrences in project descriptions
		\item \textbf{RAS} a component that computes a score based on the relevant API calls
		\item \textbf{DCS} a score based on data-flow connections between calls
	\end{enumerate}
	In this paper \citet{McMillan:2011:FRF:1985793.1986032} concludes by mentioning that the performance of search engines can be improved if those engines consider the API calls that the software uses.\\

	
	\citet{Bajracharya2012} conducted an exploratory analysis of the usage log of Koders, the first commercially available Internet-Scale code search engine, and their goal is to answer the following three questions:\\
	\begin{enumerate}
	\item \textbf{Usage}: What kind of usage behavior can we see in Koders?
	\item \textbf{Search Topics}: What are the users searching for?
	\item \textbf{Query Forms}: How are users expressing their information need in their queries?
	\end{enumerate}
	They start analyzing the usage logs, and they get the following results: 
	\begin{itemize}
	\item Most of the users did not use Koders again after using it for a day
	\item Sessions are short (a series of activities by a single user within a small duration of time constitutes a session )
	\item More than half of the sessions had no downloads
	\item There are few sessions with no search activities
	\item Queries are very short
	\item Terms in queries are quite diverse
	\item Code queries are the mostl used types of queries
	\item Code queries lead to the most of the downloads
	\end{itemize}
	they conclude the analysis by mentioning that usage behavior is similar between Koders and search on the Web, and the majority of the users don't refine their existing queries.\\

	The approaches described so far, do not take into consideration any readability or familiarity metrics. We believe that our comprehension effort metric can be integrated in in these search engines as a complement information to help the user in choosing the must relevant document.
	
	\section{Recommender Systems for Software Engineering}

	\citet{Ponz2014b} built the Prompter tool, a plug-in for the Eclipse integrated development environment (IDE), that retrieves and recommends, with push notifications, relevant Stack Overflow discussions to the developer. Prompter analyzes the code context in the IDE and searches for Stack Overflow discussions, evaluates their relevance by taking into consideration \emph{code aspects}, \emph{conceptual aspects}, \emph{community aspects}.


	The authors performed a study to evaluate to what extent the use of Prompter can be useful to developers during a development or maintenance task. In the study they selected 12 participants that have at least 3 years of experience in programming, with a maximum of 12. The authors asked the participants to perform one maintenance task and one development task, and then the authors asked them if they have used the suggestions by Prompter. Three participants answered \emph{absolutely yes}, eight \emph{more yes than no}, and two \emph{more no than yes}.


	Ponzanelli et al. concluded their work by mentioning that Prompter ranking model resulted to be effective in identifying the right discussions given a code snippet to analyze.\\


	\citet{Cubranic:2003:HRP:776816.776866} built Hipikat, a tool that helps newcomer to become productive more quickly, by recommending existing artifacts from the development that are relevant to a task that the newcomer is trying to perform. Hipikat infers links between the artifacts that may have been apparent at one time to members of the development team but that were not recorded, and then Hipikat uses this links to suggest possibly relevant parts of the group memory given information about a task a newcomer is trying to perform.\\
	The authors performed two qualitative studies, where they show that Hipikat approach helped newcomers to perform a task effectively on an unfamiliar system.\\


	\citet{Holmes:2005:USC:1062455.1062491}  built the Strathcona tool, a plug-in for the Eclipse integrated development environment (IDE), the tool extracts the structural context of the code on which a developer is working, and selects from an example repositories a set of relevant code examples to be returned using a set of structural matching heuristics.


	To understand if the structural matching heuristics can return examples that a developer finds useful, \citet{Holmes:2005:USC:1062455.1062491} performed a case study in which they asked two developers to complete four programming tasks, where Subject 1 had less than one month of Eclipse plug-in programming experience but more than eight years of Java experience. Subject 2 had over six months of Eclipse plug-in programming experience but only eighteen months of experience with Java, and neither subject knew how to implement any of the assigned tasks.
	Subject 1 completed all four tasks successfully, finding relevant examples in all cases for which appropriate examples were returned; Subject 2 completed three out of four tasks, finding relevant examples in two of the three cases. These results show that Strathcona can suggest relevant examples to developers.\\

	\citet{Ponz2017a} created LIBRA, a holistic recommender system that supports developers in their information search in the web browser. The developers are tracked during their activities in the IDE and in the web browser, which allows Libra to collect information as web search results, perused pages, and code written and modified by the developer. LIBRA uses this information to model a knowledge context of the developer and, constructs a holistic meta-information model of their contents.
	

	LIBRA is based on three important metrics :
	
	\begin{itemize}
	\item \textbf{Context Complementarity} measures the information intake provided by a resource in the current context of the developer. A low context complementarity indicates that the resource information is already a part of the context. In other words, the resource and the context are too similar, and the amount of new information that the developer can retrieve is low.
	
	\item \textbf{Result Prominence} identifies prominent results among the search engine result set. When a query matches a result, it doesn't tell us so much about the result relevance. If a result overlaps with many other results, it would be more prominent, since probably it provides diversified information in its contents.
	
	\item \textbf{Information Quantity} sums up the number of ``elements'' identified by LIBRA's meta-information system. This metric is important to distinguish which resource has more information giving two resources with a similar number of characters. If the first resource has only text and the second one has text and code, obviously the second resource contains a higher information quantity.
	\end{itemize}
	16 third year CS Bachelor students were asked to evaluate LIBRA in terms of \textit{its ability in correctly assessing for each query search result its prominence and complementarity with respect to the context, and usefulness to developers during a development or maintenance task.}\\
	The study results indicated that both prominence and complementarity indicators reflect developers’ perception of such measures, and are considered as useful indicators. Moreover, students  achieved a significantly better task completeness with LIBRA.\\

	The work that has been done in this thesis can be integrated with RSSE as LIBRA, where the comprehension effort metric can be added to the other three metrics used by LIBRA to provide developers with the most relevant documents.


\chapter{Approach}
	The volume of online resources is increasing, we can find millions of documents related to one search query. How can we suggest the most suitable one to developers? Several elements might determine which documents contain valuable information, and one of these elements is the comprehension effort. Code search has been a part of software development for decades, \citet{Singer-1997} reported in 1997 that the most frequent developer activity was code search, and most searches focus on code that is familiar to the developer \cite{Sadowski:2015}. Certainly, developers prefer to search for documents they are familiar with since they require less effort to be comprehended. We believe that the comprehension effort is strictly related to the document familiarity and readability.
     
    In the literature there are several metrics that estimate the readability of a given text, (e.g., Flesch$-$Kincaid \cite{Kincaid}, Dale$-$Chall \cite{Dale-Chall}), and other metrics that estimate a code readability \cite{Buse:2010:LMC:1850489.1850615}. In this thesis, we introduce for the first time a new approach where we combine text readability, code readability and document familiarity, to estimate a comprehension effort.\\

	
	
	\begin{figure}[H]
	\centering
	\includegraphics[width=6cm, height=10cm]{overview}
	\caption{Overall architecture of our approach}
	\label{overview}
	\end{figure}
	
	\section{Overview}

	Fig.\ref{overview} shows the overall architecture of our approach. As we can see, there are two \emph{language models}, one for code and one for text (natural language), and the whole process is divided in two phases that work in parallel: 
	\begin{itemize}

		\item \textbf{Training phase}: In this phase we train the language models on a given set of documents, where each document is broken down in two segments: code and text. To extract code and natural language from a document, we use Stormed Island Parser\footnote{\url{https://stormed.inf.usi.ch}} \cite{Ponz2015a}. The \emph{code language model} will be trained with the code segment, and the \emph{text language model} will be trained with the text segment. These language models will give us the familiarity of a given document in the assessing phase.


		\item \textbf{Assessing phase}: In this phase we give each document a \emph{Comprehension Effort} score. As a first step we use Stormed \cite{Ponz2015a} to brake down each document in two segments: code and text, then we use the language models that we trained in the training phase to estimate the familiarity. In the same time we use Flesch$-$Kincaid\cite{Kincaid} to calculate the text readability, and RayKernel \cite{Buse:2010:LMC:1850489.1850615} to calculate the code readability. In the last step we combine the familiarity and the readability to get the comprehension effort.
	
	\end{itemize}

	In the next sections we explain the training and assessing phase, and we give more details about the component that are used in the whole process.

	\section{Training Phase}

	As we have mentioned before, in the training phase we use Stormed to distinguish between code and text, and we train two language models, one for text and one for code. We explain what Stormed is and how we use it, and then we explain what a language model is, and how we train it.

	\subsection{Stormed Island Parser}
	
	 Stormed is a dataset and parser for Stack Overflow that models the posts by building a heterogeneous abstract syntax tree (H-AST) for each discussion in the data dump \cite{Ponz2015a}.
	 Moreover, Stormed can parse any given data in form of text. For example, given the following text : 
	 \begin{verbatim} With a sorted array, the condition data[c] >= 128  \end{verbatim} 
	 Stormed can identify  \emph{``With a sorted array, the condition''} as text, and \\``$data[c] >= 128$'' as code. 


	 Stormed gives us more detailed information about the code, that can be one of the following possible structured fragments:
	 \begin{itemize}
	 \item \textbf{JavaASTNode}: Java code including incomplete fragments.
	 \item \textbf{StackTracesASTNode}: Stack traces including incomplete stack trace lines.
	 \item \textbf{XMLASTNode}: XML/HTML documents, tags and elements.
	 \item \textbf{JSONASTNode}: JSON fragments.
	 \end{itemize}

	  As shown in Fig.\cref{stackOverflow} each Stack Overflow document is represented with HTML tag, Stormed extracts two types of information units:
	 	 \begin{itemize}
		\item \textbf{Natural Language Tagged Unit} is the textual part of a discussion. Text units are all fragments that are not tagged as <code> or <pre><code>.	
		\item \textbf{Code Tagged Unit} is the code part of a discussion. Structured Fragment Unit are every contents tagged as <code> or <pre><code>.
	 \end{itemize}


	\begin{figure}[htbp]
	 \centering
	\includegraphics[width=6cm,height=8cm]{stackOverflow}
	\caption{Example of Stack Overflow question with HTML tagging}
	\label{stackOverflow}
	\end{figure}

	% \begin{figure}[htbp]
	% \centering
	% \includegraphics[width=\textwidth,height=5cm]{stormed}
	% \caption{Stormed Object Model of the dataset}
	% \label{stormed}
	% \end{figure}

	The extracted units are modeled as a H-AST, which allows us to distinguish between the textual part (natural language), and the code part. Moreover, Stormed parses the code part and identifies the Java code even if the code is incomplete.

	\subsection{Language Model}

	A \textbf{Probabilistic Language Model (LM)} is a probability distribution over sequences of words. Given such a sequence of length m, it assigns a probability $P(w_{1},\ldots ,w_{m})$ to the whole sequence\footnote{\url{https://en.wikipedia.org/wiki/Language_model}}.\\
	The goal of the language model is to compute the probability of a sentence or sequence of words.
	\[P(W) = P(w_{1},w_2,w_3,w_4,w_5\dots w_n)\]
	A LM can also compute the probability of an upcoming word.
	\[P(w_5|w_1,w_2,w_3,w_4)\]
	A LM applies Markov Chain Assumption to compute $P(W)$
	\[P(w_1w_2\dots w_n) \approx \prod_{i} P(w_i|w_{i-k} \dots w_{i-1})\]
	Each component in the product is approximated
	\[P(w_i |w_1w_2\dots w_{i-1}) \approx P(w_i |w{i-k} \dots w_{i-1})\]
	\textbf{Bigram model} provides the conditional probability of a word given the previous word.
	\[P(w_i |w_1w_2 \dots w_{i-1})\approx P(w_i |w_{i-1})\]
	The bigram model can be extended to trigrams, 4-grams, 5-grams, \dots , n-grams.\\
	$P(w_i |w_{i-1}$ is the maximum likelihood estimation in a bigram where 
	\[P(w_{i}|w_{{i-1}})=\frac{count(w_{{i-1}},w_{i})} {count(w_{{i-1}})}\]
	\textbf{Example}\\
	Lets consider the following text:
	\begin{verbatim}
	Hi I am John
	Hi John I am David
	Hi I am looking for John
\end{verbatim}
	\[P(I|HI)\ = \frac{2}{3} = 0.67\]
	The probabilities of the sentence ``I like Italian food'' estimated by a bigram model is:
	\[P(like|I) \times P(Italian|like) \times P(food|Italian)= 0.00042\]
	To avoid underflow and to make multiplication faster, we use log space.
	\[log( p_1 \times p_2 \times p_3 \times p_4 ) = log (p_1) + log (p_2) + log (p_3) + log (p_4)\]



	\subsection{Training the Language Model}
	In our approach we use the 3-Gram model, we tried the 4-Gram and the 5-Gram but there was no significant difference. As we said before, we differentiate between natural language and code, therefore, we create a language model for each as we can see in Fig.\ref{trainingLm}.\\

	\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{trainingLm}
	\caption{Training Language Model overview}
	\label{trainingLm}
	\end{figure}


	\begin{itemize}
		\item \textbf{Natural Language LM training}: Once we have identify the textual part of a document, we remove the stop words, since they are common words and they add noise to the LM, then we train the \emph{Natural Language LM} with the filtered text.
		\item \textbf{Code LM training}: For the code part we use a similar approach. We use ANTLR\footnote{\url{http://www.antlr.org}} parser to parse the code and to create tokens that we model as 3-Grams, and we pass them to the LM. But before training the LM on the code we remove all separators Fig.\ref{FilterLM}.


		The reason why we decide to remove separator is because given a code with three nested loops:	

		\begin{lstlisting}
for(int a in list){
	for(int b in list){
		for(int c in list){
			System.out.println(a + b + c)
			}
		}
	}

		\end{lstlisting}

		The code ends with three curly brackets $\textbf{\}\}\}}$ that form the most popular 3-Gram since it is frequent to have a sequence of brackets or parentheses in code. We had several situation where the most familiar 3-Gram was three brackets, for this reason we decide to remove this noise by filtering the separators.

	\end{itemize}
	

	\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{FilterLM}
	\caption{Filtering stop words and separators}
	\label{FilterLM}
	\end{figure}

	\newpage

	\section{Assessing Phase}

	In the assessing phase we estimate the familiarity of a given document, then we calculate the document readability and we combine them in a comprehension effort score.


	\subsection{Familiarity Estimation}
	We define the familiarity as the estimation of likelihood of a specified character sequence. But before estimating the familiarity, we need to do the following preprocessing steps:
	\begin{itemize}
		\item \textbf{Selecting \& Filtering}: As we did in the training phase, we use Stormed Island Parser\footnote{\url{https://stormed.inf.usi.ch}} \cite{Ponz2015a} to break down a document into text and code. We remove the stop words form the text, we parse the code\footnote{\url{http://www.antlr.org}} and we remove the separators, as shown in \cref{FilterLM}
		\item \textbf{3-Grams generating }: In Fig.\ref{3gram-evaluating} we can see that each chunk of code or text is transformed to a 3-Gram model, and then the language model evaluates the familiarity of each N-Gram.
			
			% \begin{figure}[htbp]
			% \centering
			% \includegraphics[width=\textwidth]{3gram-generating}
			% \caption{3-Gram generating process}
			% \label{3gram-generating}
			% \end{figure}

			\begin{figure}[htbp]
			\centering
			\includegraphics[width=\textwidth]{3gram-evaluating}
			\caption{3-Gram evaluating process}
			\label{3gram-evaluating}
			\end{figure}	
	
	
		\item \textbf{Familiarity aggregation}: Now that we have retrieved the familiarity of each 3-Gram of a given document, we need to aggregate them in one value. The trivial way is to multiply all the probabilities to get one value, but this approach is biased by the document length.\\
		As we can see in \cref{aggregation-by-multiplication} we have 2 documents, a and b, where \emph{document b} contains twice the content of \emph{document a}. We expect that the familiarity in both documents must be the same since they have the same text, even though in \emph{document b} the text is duplicated.\\ 
			\end{itemize}
		\begin{figure}[H]
			\centering
			\includegraphics[width=\textwidth]{aggregation-by-multiplication}
			\caption{Aggregating probabilities by multiplication}
			\label{aggregation-by-multiplication}
			\end{figure}


			\begin{figure}[htbp]
			\centering
			\includegraphics[width=\textwidth]{aggregation-by-mean}
			\caption{Aggregating probabilities by mean}
			\label{aggregation-by-mean}
			\end{figure}
		
		Given a set of documents with different length, the shortest documents turns out to be the most familiar. This approach is valid if all documents have the same length, which is not the case with the documents that we are dealing with, since they have heterogeneous length. 


		We tackle this problem by introducing a new approach to aggregate the probabilities as shown in \cref{aggregation-by-mean}.
		The approach consists in 5 steps: 
		\begin{itemize}
			\item \textbf{step 1}: We select from the set the document with the smallest number of 3-Gram, say of length m.
			\item \textbf{step 2}: We split each document into chunks of length m.
			\item \textbf{step 3}: For each chunk we calculate the familiarity of its 3-Grams
			\item \textbf{step 4}: Now that all chunks have the same size, we calculate the familiarity of a chunk by multiplying the probability of all its 3-Grams
			\item \textbf{step 5}: Is the last step, we calculate the familiarity mean of each document chunks.
		\end{itemize}

	\subsection{Readability} 
		The document readability has a big impact on the comprehension effort. Two text documents can contain approximately the same information, but one text can be easy to read and the other one can be difficult to read, and it is the same for code. The following peace of code can be written in one line :

		\lstset{language=algebra,linewidth=0.95\linewidth,breaklines=true,
		basicstyle=\ttfamily,numberstyle=\tiny,escapeinside={//*}{\^^M},
		mathescape=true}
		\begin{lstlisting}
	{"name":"mkyong.com","messages":["msg 1","msg 2"],"age":100}

		\end{lstlisting}	
	But most likely developers will find it easier to read the indented code:\\

	 	\begin{lstlisting}
{
	"name":"mkyong.com",
	"messages":["msg 1","msg 2"],
	"age":100
}
	\end{lstlisting}
 
	As mentioned so far we distinguish between \emph{code readability} and \emph{text readability}. Therefore, we use different tools to calculate the readability of each. 

\subsection{Accounting for Text Readability}
	The text readability score is calculated by the \textbf{Flesch-Kincaid} \footnote{\url{https://en.wikipedia.org/wiki/Flesch–Kincaid_readability_tests}} formula:
	

	\[0.39\left({\frac  {{\mbox{total words}}}{{\mbox{total sentences}}}}\right)+11.8\left({\frac  {{\mbox{total syllables}}}{{\mbox{total words}}}}\right)-15.59\]
	A higher scores indicate that the document is easier to read, and a lower number indicates that the document is more difficult to read.\\ 
	Flesch-Kincaid score is scaled in 0-100 rage, but for convenience we normalize it to 0-1 range. The score can be interpreted as shown in the table below, Table.\ref{tab:Flesch-Kincaid}.
	
	\begin {table}[H]
	\begin{center}
    \begin{tabular}{| l | l | p{7cm} | }
    \hline
    \textbf{Score} & \textbf{School Level} & \textbf{Notes} \\ \hline
    $100.0-90.0$ & 5th grade & Very easy to read. Easily understood by an average 11-year-old student.\\ \hline
    $90.0-80.0$ & 6th grade & Easy to read. Conversational English for consumers.\\ \hline
 	$80.0-70.0$ & 7th grade & Fairly easy to read. \\ \hline
	$70.0-60.0$ & 8th \& 9th grade & Plain English. Easily understood by 13- to 15-year-old students.\\ \hline
	$60.0-50.0$ & 10th to 12th grade &  Fairly difficult to read.\\ \hline
	$50.0-30.0$ & College & Difficult to read.\\ \hline
	$30.0-0.0$ & College Graduate & Very difficult to read. Best understood by university graduates\\\hline

    \end{tabular}
	\end{center}
	\caption{Flesch-Kincaid score grade} \label{tab:Flesch-Kincaid} 
	\end{table}

\subsection{Accounting for Code Readability}
	To calculate the code readability we use \citet{Buse:2010:LMC:1850489.1850615} code readability metric, and tool\footnote{\url{http://www.arrestedcomputing.com/readability}}.
	 Buse and Weimer determined a set of code features that are predictive of readability. The features are listed in the Table.\ref{tab:Buse and Weimer}.

	\begin {table}[H]
	\begin{center}
    \begin{tabular}{ | l | }
    \hline
    \textbf{Feature Name}\\ \hline
	line length (\# characters)\\ \hline
 	\# identifiers\\ \hline
 	identifier length\\ \hline
	indentation (preceding whitespace)\\ \hline
	\#keywords\\ \hline
	 \#numbers\\ \hline
	\#comments\\ \hline
	\#periods\\ \hline
	\#commas\\ \hline
	\#spaces\\ \hline
	\#parenthesis\\ \hline
	 \#arithmetic operators\\ \hline
	 \#comparison operators\\ \hline
	 \#assignments (=)\\ \hline
	 \#branches (if)\\ \hline
	 \#loops (for, while)\\ \hline
	 \#blank lines\\ \hline
	 \#occurrences of any single character\\ \hline
	 \#occurrences of any single identifier\\ \hline
    \end{tabular}
	\end{center}
		\caption{Buse and Weimer code features} \label{tab:Buse and Weimer} 
	\end{table}

	Buse and Weimer's readability score is scaled in 0-1 range, where a higher scores indicate that the code is easier to read and a lower number indicates that the code is more difficult to read.


\section{Accounting Comprehension Effort }

To calculate the \emph{Comprehension Effort} of a given document we calculate:
\begin{itemize}
\item The familiarity of the textual parts $f_{t}$ 
\item The familiarity of the code parts $f_{c}$
\item The readability of the textual parts $r_{t}$
\item The readability of the code parts $r_{c}$ 
\end{itemize}
 
 The $r_{t}$  and $r_{c}$ are on a scale of 0 to 1,  the $f_{t}$  and $f_{c}$ are unbounded $log_{2}$ probabilities.\\
 We propose two formulas to calculate the \emph{Comprehension Effort}:

 \[\text{Comprehension Effort = }\frac{(r_{c}\times f_{c}) + (r_{t} \times f_{t})}{2} \]
 In this formula the \emph{readability} and the \emph{familiarity} have the same weight in calculating the \emph{comprehension effort}
 \[\text{Comprehension Effort = }\frac{(r_{c}\times f_{c}) + (r_{t} \times f_{t})}{r_c+r_t} \]
 In this formula the \emph{familiarity} has a higher weight, but we decided to explore the first formula in this thesis.
	
\chapter{Study Design}

	The goal of our study is to assess the LM capability to evaluate the familiarity of a given document, and to evaluate the effectiveness of our approach in assessing documents by the comprehension effort. Towards this goal, we run two tailored studies:
	

	In the first study, we select a big set of Android documents, which represents the developer knowledge context, then we train the \emph{Language Models} with these documents (the training procedure is described in Chapter 3.2). Once we trained the language model, we evaluate the familiarity of a given set of documents (testing documents) that contains Android documents and other documents that are not related to Android (the familiarity accounting is described in Chapter 3.3.1). 
		

	 In the second study we ask programmers to read a set of tutorials, and then we give them a set of documents, where some documents are related to the tutorials and some are not, and we ask them to score the documents by the comprehension effort on a scale from 1 to 5. Then we compare their score with our precomputed comprehension effort score (the comprehension effort accounting is described in Chapter 3.4).


	\section{Research Questions}
	To evaluate the effectiveness of our approach we have formulated the following research questions:
	\newpage

	 \textbf{RQ1}: \emph{Can a language model evaluate correctly the familiarity of a given document?}


		We have answered this research question by downloading 200,000 Stack Overflows' questions tagged as Android. From these 200,00 documents we select 5 training sets with different size (10, 100, 1000, 10000, 100000), and we select 1000 documents for testing.


		We extend the testing set by downloading: 
		\begin{itemize}
			\item 1000 Stack OVerflows' questions tagged as JavaScript.
			\item 1000 Stack OVerflows' questions tagged as Swift.
			\item 1000 Stack OVerflows' questions tagged as Java.
		\end{itemize}

		We create a code LM and a text LM, and we train them with the Android 10 documents training set, then we calculate the familiarity of the testing sets
		(Android, JavaScript, Swift, Java). We repeat this experiment with 100, 1000, 10000, 100000 android training sets to understand how the training set size can influence the performance. Then we plot the familiarity results.\\


		{\textbf{RQ2}: \emph{What is the accuracy of our technique in assessing documents by the comprehension effort?}


		To answer this research question we asked developers how has no experience with android to read a set of introduction tutorials to Android, then we asked them to read two Android tutorials on two specific tasks. Once they have finished reading the tutorials we asked them to score the comprehension effort of 6 documents from Stack Overflow. The comprehension effort scores are on a scale of 0 to 5, where a higher score indicates a higher effort in comprehending the documents.\\ The 6 documents from Stack Overflow are selected in the following way:
		\begin{itemize}
			\item 2 documents are Android discussions related to the tutorials
			\item 2 documents are Android discussions but they are not related to the tutorials
			\item 2 documents are Cordova discussions. (Cordova also known as PhoneGap is a mobile application development framework that enables software programmers to build applications for mobile devices using CSS3, HTML5, and JavaScript).
		\end{itemize}
		In the meanwhile we train the LMs with the tutorials that we gave to the developers and we calculate the comprehension effort of the 6 Stack Overflow documents. Then we compare our results with the developers scores.



	\section{Data Collection and Analysis}	




	\section{Replication Package}

\chapter{Results}

\chapter{Threats to Validity}

\chapter{Conclusion}

\chapter[Short title]{A chapter title which will run over two lines --- it's for
  testing purpose}


\textbf{Theorem 1 (Residue Theorem).}
Let $f$ be analytic in the region $G$ except for the isolated singularities $a_1,a_2,\ldots,a_m$. If $\gamma$ is a closed rectifiable curve in $G$ which does not pass through any of the points $a_k$ and if $\gamma\approx 0$ in $G$ then
\[
\frac{1}{2\pi i}\int_\gamma f = \sum_{k=1}^m n(\gamma;a_k) \text{Res}(f;a_k).
\]
\textbf{Theorem 2 (Maximum Modulus).}
\emph{Let $G$ be a bounded open set in $\mathbb{C}$ and suppose that $f$ is a continuous function on $G^-$ which is analytic in $G$. Then}
\[
\max\{|f(z)|:z\in G^-\}=\max \{|f(z)|:z\in \partial G \}.
\]

\section[third]{A very very long section, titled ``The third section'', with
  a rather  short text alternative (third)}



\nocite{*}

\appendix %optional, use only if you have an appendix



\backmatter

\chapter{Glossary} %optional

%\bibliographystyle{alpha}
%\bibliographystyle{dcu}
\bibliographystyle{plainnat}
\bibliography{biblio}

%\cleardoublepage
%\theindex %optional, use only if you have an index, must use
	  %\makeindex in the preamble


\end{document}
